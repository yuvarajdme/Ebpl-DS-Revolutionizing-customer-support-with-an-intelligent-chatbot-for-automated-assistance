# -*- coding: utf-8 -*-
"""8 week

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j42hBEvKoWbva1jio7tDl4HHshCa3AUv
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
# Generate sample data
X, y = make_blobs(n_samples=300, centers=4, random_state=42)
# Apply K-Means
kmeans = KMeans(n_clusters=4, random_state=42)
y_kmeans = kmeans.fit_predict(X)

# Plot the clusters
plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='viridis', alpha=0.7)
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],
            s=300, c='red', label='Centroids', marker='X')
plt.legend()
plt.title("K-Means Clustering")
plt.show()

from sklearn.cluster import AgglomerativeClustering
import scipy.cluster.hierarchy as sch
# Create dendrogram
plt.figure(figsize=(8, 5))
dendrogram = sch.dendrogram(sch.linkage(X, method='ward'))
plt.title("Dendrogram")
plt.show()
# Apply Agglomerative Clustering
hc = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')
y_hc = hc.fit_predict(X)
# Plot clusters
plt.scatter(X[:, 0], X[:, 1], c=y_hc, cmap='rainbow', alpha=0.7)
plt.title("Hierarchical Clustering")
plt.show()

from sklearn.cluster import AgglomerativeClustering
import scipy.cluster.hierarchy as sch
# Create dendrogram
plt.figure(figsize=(8, 5))
dendrogram = sch.dendrogram(sch.linkage(X, method='ward'))
plt.title("Dendrogram")
plt.show()
# Apply Agglomerative Clustering
# Removed affinity parameter as it's not required with 'ward' linkage
hc = AgglomerativeClustering(n_clusters=4, linkage='ward')
y_hc = hc.fit_predict(X)
# Plot clusters
plt.scatter(X[:, 0], X[:, 1], c=y_hc, cmap='rainbow', alpha=0.7)
plt.title("Hierarchical Clustering")
plt.show()

from sklearn.cluster import DBSCAN
# Apply DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)
y_dbscan = dbscan.fit_predict(X)
# Plot clusters
plt.scatter(X[:, 0], X[:, 1], c=y_dbscan, cmap='plasma', alpha=0.7)
plt.title("DBSCAN Clustering")
plt.show()

from sklearn.mixture import GaussianMixture
# Apply GMM
gmm = GaussianMixture(n_components=4, random_state=42)
y_gmm = gmm.fit_predict(X)
# Plot clusters
plt.scatter(X[:, 0], X[:, 1], c=y_gmm, cmap='coolwarm', alpha=0.7)
plt.title("Gaussian Mixture Model Clustering")
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.datasets import load_digits
# Load dataset (Handwritten digits)
digits = load_digits()
X = digits.data
y = digits.target
# Apply PCA (Reduce to 2 dimensions)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)
# Scatter plot of PCA results
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.7)
plt.colorbar(label="Digit Label")
plt.title("PCA on Handwritten Digits")
plt.show()

from sklearn.manifold import TSNE
# Apply t-SNE (Reduce to 2D)
tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(X)
# Scatter plot of t-SNE results
plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='Spectral', alpha=0.7)
plt.colorbar(label="Digit Label")
plt.title("t-SNE on Handwritten Digits")
plt.show()

import tensorflow as tf
from tensorflow import keras
# Define Autoencoder Model
input_dim = X.shape[1]
encoding_dim = 32  # Reduced dimension
# Encoder
input_layer = keras.layers.Input(shape=(input_dim,))
encoded = keras.layers.Dense(encoding_dim, activation='relu')(input_layer)
# Decoder
decoded = keras.layers.Dense(input_dim, activation='sigmoid')(encoded)

# Compile Autoencoder
autoencoder = keras.models.Model(input_layer, decoded)
autoencoder.compile(optimizer='adam', loss='mse')
# Train Autoencoder
autoencoder.fit(X, X, epochs=20, batch_size=256, shuffle=True, verbose=1)
# Get encoded representation
encoder = keras.models.Model(input_layer, encoded)
X_autoencoded = encoder.predict(X)
# Scatter plot of Autoencoder results (first 2 dimensions)
plt.scatter(X_autoencoded[:, 0], X_autoencoded[:, 1], c=y, cmap='plasma', alpha=0.7)
plt.colorbar(label="Digit Label")
plt.title("Autoencoder Representation of Digits")
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs

# Generate synthetic data (300 samples, 4 clusters)
X, y = make_blobs(n_samples=300, centers=4, random_state=42)

# Apply K-Means clustering
kmeans = KMeans(n_clusters=4, random_state=42)
y_kmeans = kmeans.fit_predict(X)

# Plot clusters
plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='viridis', alpha=0.7)
# Plot centroids
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],
            s=300, c='red', marker='X', label="Centroids")
plt.legend()
plt.title("K-Means Clustering")
plt.show()

wcss = []  # Within-cluster sum of squares
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)

# Plot the elbow graph
plt.plot(range(1, 11), wcss, marker='o', linestyle='--')
plt.xlabel("Number of Clusters (K)")
plt.ylabel("WCSS (Within-Cluster Sum of Squares)")
plt.title("Elbow Method for Optimal K")
plt.show()

import matplotlib.pyplot as plt
import scipy.cluster.hierarchy as sch
from sklearn.cluster import AgglomerativeClustering
from sklearn.datasets import make_blobs

# Generate synthetic data
X, y = make_blobs(n_samples=300, centers=4, random_state=42)

# Plot the dendrogram
plt.figure(figsize=(8, 5))
dendrogram = sch.dendrogram(sch.linkage(X, method='ward'))
plt.title("Dendrogram")
plt.xlabel("Data Points")
plt.ylabel("Euclidean Distance")
plt.show()

# Apply Agglomerative Clustering
hc = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')
y_hc = hc.fit_predict(X)

# Apply Agglomerative Clustering
# Removed affinity parameter as it's not required with 'ward' linkage
hc = AgglomerativeClustering(n_clusters=4, linkage='ward')
y_hc = hc.fit_predict(X)

# Plot clusters
plt.scatter(X[:, 0], X[:, 1], c=y_hc, cmap='rainbow', alpha=0.7)
plt.title("Hierarchical Clustering")
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.datasets import load_digits
from sklearn.preprocessing import StandardScaler

# Load the digits dataset
digits = load_digits()
X = digits.data  # Features (64-dimensional)
y = digits.target  # Labels (digits 0-9)

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# Scatter plot of PCA results
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.7)
plt.colorbar(label="Digit Label")
plt.title("PCA: Handwritten Digits (2D Projection)")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.show()

# Explained variance ratio
explained_variance = pca.explained_variance_ratio_
print(f"Explained Variance (PC1 + PC2): {sum(explained_variance) * 100:.2f}%")

# Compute cumulative explained variance
pca_full = PCA().fit(X_scaled)
cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)
# Plot cumulative variance
plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--')
plt.xlabel("Number of Principal Components")
plt.ylabel("Cumulative Explained Variance")
plt.title("PCA: Choosing the Optimal Number of Components")
plt.axhline(y=0.95, color='r', linestyle='--')  # 95% threshold
plt.show()

optimal_components = np.argmax(cumulative_variance >= 0.95) + 1
print(f"Optimal Number of Components for 95% Variance: {optimal_components}")